{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå What is Boosting in Ensemble Learning?\n",
    "\n",
    "**Boosting** is an **Ensemble Learning** technique that aims to improve model performance by **correcting errors from previous models** iteratively.  \n",
    "Unlike **Bagging**, which reduces variance, **Boosting** focuses on reducing bias and improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è How Does Boosting Work?\n",
    "Boosting trains multiple models **sequentially** instead of in parallel (like Bagging). The process follows these steps:\n",
    "\n",
    "1Ô∏è‚É£ **Train the First Model**  \n",
    "   - A weak model is trained on the original dataset.\n",
    "\n",
    "2Ô∏è‚É£ **Error Correction**  \n",
    "   - **Higher weights** are assigned to misclassified samples.  \n",
    "   - The next model is trained with a **focus on correcting previous errors**.\n",
    "\n",
    "3Ô∏è‚É£ **Final Prediction**  \n",
    "   - All models are combined to make a final, more accurate prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Advantages of Boosting\n",
    "‚úÖ **Improves accuracy** significantly by focusing on previous mistakes.  \n",
    "‚úÖ **Reduces bias**, making it suitable for weak learners like **Logistic Regression**.  \n",
    "‚úÖ **Works well with imbalanced datasets**.\n",
    "\n",
    "‚ùå **More prone to overfitting** if hyperparameters are not tuned properly.  \n",
    "‚ùå **Computationally expensive**, as models are trained sequentially.  \n",
    "‚ùå **Sensitive to noise**, as it may overfocus on misclassified samples.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Popular Boosting Algorithms\n",
    "### 1Ô∏è‚É£ **AdaBoost (Adaptive Boosting)**\n",
    "- Assigns **different weights** to samples based on misclassification.  \n",
    "- Later models **correct mistakes** from earlier ones.  \n",
    "- Works well with **simple models like Decision Stumps**.\n",
    "\n",
    "### 2Ô∏è‚É£ **Gradient Boosting (GB)**\n",
    "- Optimizes a **loss function** using **Gradient Descent**.  \n",
    "- More efficient than **AdaBoost** for large datasets.  \n",
    "- **XGBoost and LightGBM** are advanced versions of this method.\n",
    "\n",
    "### 3Ô∏è‚É£ **XGBoost (Extreme Gradient Boosting)**\n",
    "- An improved version of Gradient Boosting with better speed and accuracy.  \n",
    "- Uses **pruning** to prevent overfitting.  \n",
    "- One of the most popular algorithms in **Kaggle competitions**.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Example: Implementing Boosting in Python\n",
    "You can use **AdaBoost** with **Scikit-learn** like this:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoost model with simple Decision Trees\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learners (stumps)\n",
    "    n_estimators=50,  # Number of models\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = adaboost.predict(X_test)\n",
    "print(f'AdaBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
